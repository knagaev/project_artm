{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e64e67-264d-4ae3-b698-88210cbd4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples/train_artm.py\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from artm_lib.artm.model import ARTM\n",
    "from artm_lib.artm.model2 import FullEM_ARTM\n",
    "from artm_lib.data.collators import ARTMCollator\n",
    "from artm_lib.data.dataset import ARTMDatasetParquet\n",
    "from artm_lib.preprocessing.tokenizer import simple_tokenizer\n",
    "from artm_lib.preprocessing.vocabulary import build_vocab_and_index_from_parquet\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5369196e-2092-477f-ae10-3b976aaec5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scanning: input.parquet\n",
      "Vocab size: 8074 | Documents: 2225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Загрузка словаря и данных\n",
    "token_to_id, doc_index = build_vocab_and_index_from_parquet(\n",
    "    parquet_dir_str=r\".\\data\\parquets\", tokenizer=simple_tokenizer, min_df=5\n",
    ")\n",
    "vocab = [\"\"] * len(token_to_id)\n",
    "for token, idx in token_to_id.items():\n",
    "    vocab[idx] = token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52905ae6-95ae-44a5-8d29-bcca08f8d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DataLoader\n",
    "dataset = ARTMDatasetParquet(\n",
    "    doc_index=doc_index,\n",
    "    token_to_id=token_to_id,\n",
    "    text_column=\"Description\",\n",
    "    tokenizer=simple_tokenizer,\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset, batch_size=1, collate_fn=ARTMCollator(len(token_to_id)), num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dced0e-26d6-4cc9-a000-abfd1f41896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_ids, bow in loader:\n",
    "    print(f\"doc_ids: {doc_ids}\")\n",
    "    print(f\"Batch shape: {bow.shape}\")\n",
    "    print(f\"Non-zero entries: {bow.nnz}\")\n",
    "    print(f\"bow: {bow}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8d62295-0133-4618-9274-51b1c7d0141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8074)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c5bc2fa-7dc2-43e2-abdd-f0fa248e8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(x):\n",
    "    x = np.maximum(x, np.zeros_like(x))\n",
    "    norm = x.sum(axis=0)    #// суммирование по столбцам (получается одна строка)\n",
    "    if norm.any() == 0:\n",
    "        print(f'{norm=}')\n",
    "    _eps = 1e-4\n",
    "    x = np.where(norm > _eps, x / norm, np.zeros_like(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e82290fc-ad65-4cea-8174-8a468991ef6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1,1,8074) into shape (1,8074,64592)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m theta_td = np.full(shape=(n_topics, ), fill_value=\u001b[32m1.\u001b[39m / n_topics) \u001b[38;5;66;03m#np.array([0.5, 0.5])\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     n_tdw = \u001b[43mn_dw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphi_wt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_td\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     new_theta_td = _norm(np.sum(n_tdw, axis=\u001b[32m0\u001b[39m))\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.linalg.norm(new_theta_td - theta_td) < tol:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ARTM\\project_artm\\.venv\\Lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:226\u001b[39m, in \u001b[36mmatrix.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, (N.ndarray, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# This promotes 1-D vectors to row vectors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isscalar(other) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(other, \u001b[33m'\u001b[39m\u001b[33m__rmul__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m N.dot(\u001b[38;5;28mself\u001b[39m, other)\n",
      "\u001b[31mValueError\u001b[39m: could not broadcast input array from shape (1,1,8074) into shape (1,8074,64592)"
     ]
    }
   ],
   "source": [
    "#data = X.copy()\n",
    "data = DataLoader(\n",
    "    dataset, batch_size=1, collate_fn=ARTMCollator(len(token_to_id)), num_workers=0\n",
    ")\n",
    "vocabulary = vocab.copy()\n",
    "\n",
    "n_topics = 10\n",
    "tol = 1e-3\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "phi_wt = rng.uniform(size=(len(vocabulary), n_topics))\n",
    "\n",
    "for i in range(100):\n",
    "    #print(i)\n",
    "    n_wt = np.zeros(shape=(len(vocabulary), n_topics))\n",
    "    for doc_id, n_dw in data:\n",
    "        n_dw = n_dw.todense()\n",
    "        theta_td = np.full(shape=(n_topics, ), fill_value=1. / n_topics) #np.array([0.5, 0.5])\n",
    "        for _ in range(100):\n",
    "            n_tdw = n_dw[:, None] * _norm((phi_wt.T * theta_td[:, None]).T)\n",
    "            new_theta_td = _norm(np.sum(n_tdw, axis=0))\n",
    "            if np.linalg.norm(new_theta_td - theta_td) < tol:\n",
    "                theta_td = new_theta_td\n",
    "                break\n",
    "            theta_td = new_theta_td\n",
    "        n_wt += n_tdw\n",
    "    new_phi_wt = _norm(n_wt)    \n",
    "\n",
    "    diff_phi_wt = np.linalg.norm(new_phi_wt - phi_wt, 'fro')\n",
    "    print(diff_phi_wt)\n",
    "    if diff_phi_wt < tol:\n",
    "        phi_wt = new_phi_wt\n",
    "        break\n",
    "    phi_wt = new_phi_wt\n",
    "\n",
    "print(phi_wt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffaffb4-c27c-45de-bab3-82d3e37bc25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
