{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e3f8a-1d3e-4443-a26e-ee1e9a82dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095230e-37c4-43fc-b68d-bfa34f53aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"csv_dir\": r\".\",\n",
    "    \"text_column\": \"Description\",\n",
    "    \"min_df\": 5,          # опционально: фильтрация редких токенов\n",
    "    \"max_df\": 0.95,\n",
    "    \"tokenizer\": your_custom_tokenizer,  # Callable[[str], List[str]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9d256-7384-450f-951d-435581a77f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "class CorpusIndexer:\n",
    "    def __init__(self, csv_dir: str, text_column: str = \"text\"):\n",
    "        self.csv_dir = Path(csv_dir)\n",
    "        self.text_column = text_column\n",
    "        self.doc_paths = []      # list of (filepath, line_idx)\n",
    "        self.token_freq = Counter()\n",
    "        self._scanned = False\n",
    "\n",
    "    def scan_corpus(self, tokenizer):\n",
    "        \"\"\"Проход по всем CSV для сбора частот токенов и индексации документов.\"\"\"\n",
    "        self.doc_paths.clear()\n",
    "        self.token_freq.clear()\n",
    "        \n",
    "        for csv_file in sorted(self.csv_dir.glob(\"*.csv\")):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            for line_idx, row in df.iterrows():\n",
    "                text = row[self.text_column]\n",
    "                tokens = tokenizer(text)\n",
    "                self.token_freq.update(tokens)\n",
    "                self.doc_paths.append((str(csv_file), line_idx))\n",
    "        self._scanned = True\n",
    "\n",
    "    def build_vocab(self, min_df: int = 1, max_df: float = 1.0):\n",
    "        assert self._scanned, \"Call scan_corpus first!\"\n",
    "        total_docs = len(self.doc_paths)\n",
    "        min_count = min_df\n",
    "        max_count = int(max_df * total_docs)\n",
    "\n",
    "        # Фильтрация\n",
    "        filtered_tokens = [\n",
    "            token for token, freq in self.token_freq.items()\n",
    "            if min_count <= freq <= max_count\n",
    "        ]\n",
    "        # Сортируем для детерминизма\n",
    "        filtered_tokens.sort()\n",
    "        token_to_id = {token: i for i, token in enumerate(filtered_tokens)}\n",
    "        return token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c9fa1-1b59-4653-be2b-1bb9cedece1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, doc_paths, token_to_id, text_column, tokenizer):\n",
    "        self.doc_paths = doc_paths\n",
    "        self.token_to_id = token_to_id  # замороженный dict\n",
    "        self.text_column = text_column\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, line_idx = self.doc_paths[idx]\n",
    "        df = pd.read_csv(file_path)\n",
    "        text = df.iloc[line_idx][self.text_column]\n",
    "        tokens = self.tokenizer(text)\n",
    "        # Маппинг в ID, игнорируем неизвестные\n",
    "        token_ids = [self.token_to_id[t] for t in tokens if t in self.token_to_id]\n",
    "        return idx, token_ids  # idx = глобальный ID документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab15bc5-673a-4153-8b41-ae1ca773828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artm_collate_fn(batch, vocab_size: int):\n",
    "    doc_ids = []\n",
    "    rows, cols, data = [], [], []\n",
    "    current_row = 0\n",
    "\n",
    "    for doc_id, token_ids in batch:\n",
    "        doc_ids.append(doc_id)\n",
    "        if not token_ids:\n",
    "            current_row += 1\n",
    "            continue\n",
    "        counts = Counter(token_ids)\n",
    "        for token_id, cnt in counts.items():\n",
    "            if 0 <= token_id < vocab_size:  # защита\n",
    "                rows.append(current_row)\n",
    "                cols.append(token_id)\n",
    "                data.append(cnt)\n",
    "        current_row += 1\n",
    "\n",
    "    bow_matrix = csr_matrix(\n",
    "        (data, (rows, cols)),\n",
    "        shape=(len(batch), vocab_size),\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    return doc_ids, bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc61d37-ab39-49ad-925b-f00b9664fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Индексация и словарь\n",
    "indexer = CorpusIndexer(\".\", text_column=\"Description\")\n",
    "indexer.scan_corpus(tokenizer=my_tokenizer)\n",
    "token_to_id = indexer.build_vocab(min_df=5, max_df=0.9)\n",
    "\n",
    "# 2. Датасет и загрузчик\n",
    "dataset = ARTMDataset(\n",
    "    doc_paths=indexer.doc_paths,\n",
    "    token_to_id=token_to_id,\n",
    "    text_column=\"text\",\n",
    "    tokenizer=my_tokenizer\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    collate_fn=lambda b: artm_collate_fn(b, vocab_size=len(token_to_id)),\n",
    "    num_workers=6,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 3. Твоя ARTM-модель\n",
    "for epoch in range(num_epochs):\n",
    "    for doc_ids, bow in loader:\n",
    "        # bow — scipy.sparse.csr_matrix\n",
    "        # Передаёшь в свою реализацию ARTM\n",
    "        your_artm_model.update(bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
